{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costruzione del dataset di apprendimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traceback import print_tb\n",
    "import networkx as nx\n",
    "import csv\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import time\n",
    "\n",
    "#credenziali per usare le api di spotipy\n",
    "spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id=\"85aa8b30835e45a3b99623b2797df916\",client_secret=\"ecb8aa6ad9d7431db74fb99af51dec93\"))\n",
    "\n",
    "#il file global.csv contiene i link delle classifiche top 50 di ogni paese interessato, oltre al link dela classifica top 50 global\n",
    "people = list()\n",
    "with open('global.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        people.append(row)\n",
    "\n",
    "#elimino la voce corrispondente al link della playlist top 50 global\n",
    "del people[0]\n",
    "dataset = list()\n",
    "\n",
    "j = 0\n",
    "aux = {}\n",
    "pausa = 0\n",
    "pausaPaese = 0\n",
    "\n",
    "#siccome spotify banna per 24 ore se si fanno troppe richieste in poco tempo, mi fermo per 35 secondi ogni 8 paesi considerati e, per ogni paese, mi fermo per 35 secondi ogni 8 canzoni considerate\n",
    "for s in range(0,73):\n",
    "    pausaPaese += 1\n",
    "    if pausaPaese % 8 == 0:\n",
    "        time.sleep(35)\n",
    "    #per capire se sono passato al prossimo paese, questa e un'altra print mi sono utili per capire se sono stato bannato oppure no, perchÃ¨ altrimenti inizia a stampare \"Max Retries Reached\" e non posso usare le api per 24 ore\n",
    "    print('prossimo paese')\n",
    "    #ricavo la playlist top 50 del paese interessato\n",
    "    playlist = spotify.playlist(people[s][\"Link\"])\n",
    "    #e ne prendo tutte le canzoni\n",
    "    songs = spotify.playlist_items(playlist[\"id\"]) \n",
    "    #considero le prime 25 canzoni (ho avuto problemi di ban anche fermandomi ogni 35 secondi, ho stabilito che 25 canzoni per paese era il numero massimo di richieste che potevo fare)\n",
    "    for i in range (0,26):\n",
    "        pausa += 1\n",
    "        if pausa % 8 == 0:\n",
    "            time.sleep(35)\n",
    "        #per capire se sto scorrendo le canzoni\n",
    "        print('dentro al ciclo')\n",
    "        #ricavo isrc, uri della canzone e degli artisti e i generi degli artisti\n",
    "        isrc = songs[\"items\"][i][\"track\"][\"external_ids\"][\"isrc\"]\n",
    "        track_uri = songs[\"items\"][i][\"track\"][\"uri\"]\n",
    "        artist_uri = songs[\"items\"][i][\"track\"][\"artists\"][0][\"uri\"]\n",
    "        genres = spotify.artist(artist_uri)[\"genres\"]\n",
    "        info = spotify.audio_features(track_uri)[0]\n",
    "        \n",
    "        #prelevo la danceability della canzone\n",
    "        danceability = 0\n",
    "        try:\n",
    "            danceability = info['danceability']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #prelevo l'energy della canzone\n",
    "        energy = 0\n",
    "        try:\n",
    "            energy = info['energy']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #prelevo la speechiness della canzone\n",
    "        speechiness = 0\n",
    "        try:\n",
    "            speechiness = info['speechiness']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #prelevo l'acousticness della canzone\n",
    "        acousticness = 0\n",
    "        try:\n",
    "            acousticness = info['acousticness']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #prelevo l'instrumentalness della canzone\n",
    "        instrumentalness = 0\n",
    "        try:\n",
    "            instrumentalness = info['instrumentalness']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #prelevo la valence della canzone\n",
    "        valence = 0\n",
    "        try:\n",
    "            valence = info['valence']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #prelevo il tempo in bpm della canzone\n",
    "        tempo = 0\n",
    "        try:\n",
    "            tempo = info['tempo']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #salvo queste informazioni in aux\n",
    "        aux[j] = {\"paese\": people[s][\"Name\"],\"isrc\":isrc,\"uri\":track_uri,\"genres\":genres,\"danceability\":danceability,\"energy\":energy,\"speechiness\":speechiness,\"acousticness\":acousticness,\"instrumentalness\":instrumentalness,\"valence\":valence,\"tempo\":tempo}\n",
    "        j = j+1\n",
    "pausaPaese = 0\n",
    "print('uscito dal ciclo')\n",
    "print(aux)\n",
    "\n",
    "#infine, scrivo il tutto in un file csv\n",
    "with open('machine-global01.csv', 'w+', newline='') as file:\n",
    "    writer = csv.DictWriter(file,fieldnames=[\"paese\",\"isrc\",\"uri\",\"genres\",\"danceability\",\"energy\",\"speechiness\",\"acousticness\",\"instrumentalness\",\"valence\",\"tempo\"])\n",
    "    writer.writeheader()\n",
    "    for i in range (j) :\n",
    "        print('scrivo il file')\n",
    "        writer.writerow(aux[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applicazione della regressione logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from unittest import result\n",
    "from joblib import PrintTime\n",
    "\n",
    "# Data management\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Data preprocessing and trasformation (ETL)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, FunctionTransformer, Binarizer, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml, load_iris, make_moons, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Math and Stat modules\n",
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "from random import choice\n",
    "\n",
    "# Supervised Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, StratifiedKFold, RepeatedKFold, ShuffleSplit, StratifiedShuffleSplit, learning_curve, validation_curve\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve, accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset_link_prediction = pd.read_csv('machine-global-mergedfix2 - Copia.csv')\n",
    "\n",
    "generi = set()\n",
    "with open('machine-global-mergedfix2 - Copia.csv', 'r') as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    for line in csv_reader:\n",
    "        s=line[3].replace(\"[\",\"\")\n",
    "        s=s.replace(\"]\",\"\")\n",
    "        s=s.replace(\"'\",\"\")\n",
    "        genres = s.split(\",\")\n",
    "        for genre in genres:\n",
    "            if len(genre) > 0 and genre[0] == \" \":\n",
    "                genre = genre[1:]\n",
    "            generi.add(genre)\n",
    "\n",
    "for genere in generi:\n",
    "    dataset_link_prediction[genere] = 0\n",
    "\n",
    "#print(dataset_link_prediction)\n",
    "\n",
    "riga = -1\n",
    "with open('machine-global-mergedfix2 - Copia.csv', 'r') as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    for line in csv_reader:\n",
    "        if line[0] != 'paese':\n",
    "            riga = riga+1\n",
    "            s=line[3].replace(\"[\",\"\")\n",
    "            s=s.replace(\"]\",\"\")\n",
    "            s=s.replace(\"'\",\"\")\n",
    "            genres = s.split(\",\")\n",
    "            for genre in genres:\n",
    "                if len(genre) > 0 and genre[0] == \" \":\n",
    "                    genre = genre[1:]\n",
    "                dataset_link_prediction.loc[riga,genre] = 1\n",
    "\n",
    "X = dataset_link_prediction.iloc[:, 4:] #feature matrix\n",
    "y = dataset_link_prediction['paese'] # label\n",
    "\n",
    "#print(dataset_link_prediction.loc[1,'art rock'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "clf_logreg = LogisticRegression()\n",
    "clf_logreg.fit(X_train_minmax, y_train)\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "y_predicted_lr = clf_logreg.predict(X_test_minmax)\n",
    "\n",
    "print(classification_report(y_test,y_predicted_lr))\n",
    "\n",
    "print(accuracy_score(y_test, y_predicted_lr))\n",
    "#print(len(set(y_test) - set(y_predicted_lr)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
