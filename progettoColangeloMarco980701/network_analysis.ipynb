{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ricavare vari indici statistici e di network science dai dati ricavati col data gathering e costruire le community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m14\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#with open(f'global_genres0{z}.pkl', 'rb') as f:\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m#    grafo = pickle.load(f)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m#with open(f'global{z}.pkl', 'rb') as f:\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#    grafo = pickle.load(f)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#apro il grafo relativo al giorno in cui ho fatto data gathering\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviral\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 21\u001b[0m         grafo \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(nx\u001b[38;5;241m.\u001b[39mdensity(grafo))\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    with open(f'viral{z}.pkl', 'rb') as f:\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m        grafo = pickle.load(f)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m        \n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx.algorithms.community as community\n",
    "import scipy.stats as sp\n",
    "import pickle\n",
    "#con questo codice creo un file txt in cui, per ogni giorno di data gathering, sono riportati alcuni indici utili ai fini del progetto, e inoltre costruisco le community usando l'algoritmo di louvain\n",
    "\n",
    "#qui decido se fare analisi dei dati delle classifiche viral o global\n",
    "#with open('viral.txt', 'w') as file:\n",
    "with open('viral.txt', 'w') as file:   \n",
    "\n",
    "    #itero sui 15 giorni di data gathering\n",
    "    for z in range (1,16,14):\n",
    "        #with open(f'global_genres0{z}.pkl', 'rb') as f:\n",
    "        #    grafo = pickle.load(f)\n",
    "        #with open(f'global{z}.pkl', 'rb') as f:\n",
    "        #    grafo = pickle.load(f)\n",
    "        #apro il grafo relativo al giorno in cui ho fatto data gathering\n",
    "        with open(f'viral{z}.pkl', 'rb') as f:\n",
    "            grafo = pickle.load(f)\n",
    "            print(nx.density(grafo))\n",
    "                \n",
    "        '''\n",
    "        with open(f'viral{z}.pkl', 'rb') as f:\n",
    "            grafo = pickle.load(f)\n",
    "        '''        \n",
    "        #estraggo i gradi dei nodi, il numero di archi, il grado medio pesato e non pesato, la mediana dei gradi, la moda dei gradi e calcolo la densità della rete\n",
    "        degree = dict(grafo.degree(weight = 'weight'))\n",
    "        campione_grado = list(degree.values())\n",
    "        file.write(f'--DATA GATHERING 1{z} APRILE \\n')\n",
    "        file.write(f'Numero di archi: {grafo.size()} \\n')\n",
    "        file.write(f'Grado medio non pesato: {(grafo.size()*2)/grafo.order()} \\n')\n",
    "        degree = dict(grafo.degree(weight = 'weight'))\n",
    "        file.write(f'Grado medio pesato: {np.mean(list(degree.values()))} \\n')\n",
    "        file.write(f'Mediana: {np.median(campione_grado)} \\n')\n",
    "        file.write(f'Moda: {sp.mode(campione_grado)} \\n')\n",
    "        file.write(f'Densità: {(2*grafo.size())/(grafo.order()*(grafo.order()-1))} \\n')\n",
    "\n",
    "        #prendo il grado massimo\n",
    "        max_degree = max(degree.values())\n",
    "        max_country = ''\n",
    "        #e cerco il paese corrispondente al grado massimo\n",
    "        for k,v in grafo.degree(weight = 'weight'):\n",
    "            if v == max_degree:\n",
    "                max_country = k\n",
    "                break\n",
    "        file.write(f'Nodo grado massimo: {grafo.nodes.data(\"label\")[max_country]} valore: {max_degree} \\n')\n",
    "\n",
    "        #stessa cosa per il grado minimo\n",
    "        min_degree = min(degree.values())\n",
    "        min_country = ''\n",
    "        for k,v in grafo.degree(weight = 'weight'):\n",
    "            if v == min_degree:\n",
    "                min_country = k\n",
    "                break\n",
    "        file.write(f'Nodo grado minimo: {grafo.nodes.data(\"label\")[min_country]} valore: {min_degree} \\n')\n",
    "\n",
    "        #ricavo l'arco di peso massimo tra due nodi del grafo\n",
    "        edges = list(grafo.edges.data(\"weight\"))\n",
    "        max_archi = (sorted(edges, key=lambda x:x[2], reverse=True))[:5]\n",
    "        for i,j,k in max_archi:\n",
    "            file.write(f'Arco peso maggiore tra: {grafo.nodes.data(\"label\")[i]} e {grafo.nodes.data(\"label\")[j]} con valore: {k} \\n')\n",
    "\n",
    "        #e quello di peso minimo\n",
    "        min_archi = (sorted(edges, key=lambda x:x[2], reverse=True))[len(edges)-5:]\n",
    "        for i,j,k in min_archi:\n",
    "            file.write(f'Arco peso minore tra: {grafo.nodes.data(\"label\")[i]} e {grafo.nodes.data(\"label\")[j]} con valore: {k} \\n')\n",
    "\n",
    "\n",
    "        degree = dict(grafo.degree(weight = 'weight')) \n",
    "        campione_grado = list(degree.values())\n",
    "        percentile_95 = np.percentile(campione_grado,95)\n",
    "        #cerco i nodi hub considerando quelli che hanno grado maggiore del 95° percentile del grado dei nodi\n",
    "        hub_nodi = [k for k,v in degree.items() if v>= percentile_95]\n",
    "        file.write(f'Hub:')\n",
    "        for k in hub_nodi:\n",
    "            file.write(f' {grafo.nodes.data(\"label\")[k]}')\n",
    "\n",
    "        #vedo se il grafo è connesso\n",
    "        file.write(f'\\nIl grafo è connesso: {nx.is_connected(grafo)}')\n",
    "\n",
    "        #calcolo la transitività\n",
    "        file.write(f'\\nTransitività: {nx.transitivity(grafo)}')\n",
    "        #calcolo l'assortatività del grado dei nodi\n",
    "        file.write(f'\\nAssortatività grado nodi: {nx.degree_assortativity_coefficient(grafo)}')\n",
    "        \n",
    "        d = dict(grafo.nodes(data=\"label\"))\n",
    "        \n",
    "        communities = nx.community.louvain_communities(grafo, seed = 67, resolution=1.1)\n",
    "        for c in communities:\n",
    "            print(c)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        #ricavo le eigenvector centrality\n",
    "        eigen_con_pesi = nx.eigenvector_centrality(grafo, weight='weight')\n",
    "        max5_pesi = sorted(eigen_con_pesi, key=eigen_con_pesi.get, reverse=True)[:5]\n",
    "        min5_pesi = sorted(eigen_con_pesi, key=eigen_con_pesi.get, reverse=True)[67:]\n",
    "        #e cerco quelle minime e massime\n",
    "        file.write(f'\\nEigenvector massima:')\n",
    "        for k in max5_pesi:\n",
    "            file.write(f' {grafo.nodes.data(\"label\")[k]}')\n",
    "        file.write(f'\\nEigenvector minima:')\n",
    "        for k in min5_pesi:\n",
    "            file.write(f' {grafo.nodes.data(\"label\")[k]}')\n",
    "\n",
    "        file.write('---------------------\\n')\n",
    "\n",
    "        #uso l'algoritmo di louvain per trovare le community\n",
    "        l_coms = community.louvain_communities(grafo, seed=67, resolution = 1.1)\n",
    "        community_louvain = {i:e for i, e in enumerate(l_coms)}\n",
    "        for i, com in community_louvain.items():\n",
    "            for n in com:\n",
    "                grafo.nodes[n]['com'] = i \n",
    "        #e salvo le communjty in un file gexf\n",
    "        #nx.write_gexf(grafo, f'viral_community0{z}.gexf')\n",
    "        nx.write_gexf(grafo, f'global_community0{z}.gexf')\n",
    "        #z = z + 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinare i generi più e meno presenti nelle classifiche top e viral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "# Carica un grafo da un file .pkl\n",
    "def load_graph_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "    return graph\n",
    "\n",
    "# Costruisce un dizionario delle occorrenze dei nodi nei diversi grafi\n",
    "def build_node_count_dict():\n",
    "    node_count = {}\n",
    "    for i in range(1, 16):\n",
    "        #filename = f\"viral_genres0{i}.pkl\"\n",
    "        filename = f\"global_genres0{i}.pkl\"\n",
    "\n",
    "        graph = load_graph_from_pickle(filename)\n",
    "        nodes = list(graph.nodes())\n",
    "        for node in nodes:\n",
    "            node_count[node] = node_count.get(node, 0) + 1\n",
    "    return node_count\n",
    "\n",
    "# Esempio di utilizzo\n",
    "if __name__ == \"__main__\":\n",
    "    # Costruisce il dizionario delle occorrenze dei nodi\n",
    "    node_count = build_node_count_dict()\n",
    "    \n",
    "    # Ordina il dizionario per valore (occorrenze)\n",
    "    sorted_node_count = dict(sorted(node_count.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    # Stampa il dizionario ordinato\n",
    "    print(\"Dizionario delle occorrenze dei nodi (ordinato per valore):\")\n",
    "    for node, count in sorted_node_count.items():\n",
    "        print(node, \":\", count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione del grafico ECCDF per la distribuzione del grado nei vari giorni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import pickle\n",
    "import os\n",
    "from matplotlib import font_manager as fm, rcParams\n",
    "import matplotlib as mpl\n",
    "\n",
    "#apro tutti i file pkl corrispondenti ai grafi costruiti col data gathering\n",
    "for z in range (1,16):\n",
    "    giorno = 11\n",
    "    with open(f'global{z}.pkl', 'rb') as f:\n",
    "    #with open(f'viral{z}.pkl', 'rb') as f:\n",
    "        grafo = pickle.load(f)\n",
    "        degree = dict(grafo.degree(weight = 'weight'))\n",
    "        campione_degree = list(degree.values())\n",
    "\n",
    "        min_t = min(campione_degree)\n",
    "        max_t = max(campione_degree)\n",
    "\n",
    "        count_rete, bins_rete = np.histogram(campione_degree, bins = np.arange(min_t,max_t+2))\n",
    "\n",
    "        pdf_rete = count_rete / grafo.order()\n",
    "\n",
    "        cdf_fb = ECDF(campione_degree)\n",
    "        x = np.unique(campione_degree)\n",
    "        y = cdf_fb(x)\n",
    "\n",
    "        # Visualizzazione\n",
    "        fig_cdf_function = plt.figure(figsize=(21,9))\n",
    "        fig_cdf_function.set_facecolor('#1c1e21')\n",
    "        assi = fig_cdf_function.gca()\n",
    "        assi.set_facecolor('#1c1e21')\n",
    "\n",
    "        fpath = os.path.join(mpl.get_data_path(), \"fonts/ttf/DejaVuSerif.ttf\")\n",
    "        prop = fm.FontProperties(fname=fpath)\n",
    "        fname = os.path.split(fpath)[1]\n",
    "\n",
    "        assi.set_xlabel('X-axis')\n",
    "        assi.set_ylabel('Y-axis')\n",
    "\n",
    "        assi.spines['left'].set_color('white')  \n",
    "        assi.spines['right'].set_color('white')         # setting up Y-axis tick color to red\n",
    "        assi.spines['top'].set_color('white')\n",
    "        assi.spines['bottom'].set_color('white')   \n",
    "        assi.tick_params(colors='white', which='both')\n",
    "\n",
    "        assi.loglog(x,1-y,color = '#00d95a', linestyle = ':', marker= 'o',ms = 10)\n",
    "        plt.title(f'Top {giorno+z} aprile '.format(fname), fontproperties=prop, size = 30, color='#00d95a')\n",
    "        #plt.title(f'Viral {giorno+z} aprile'.format(fname), fontproperties=prop, size = 30, color='#00d95a')\n",
    "        assi.set_xlabel('Grado'.format(fname), fontproperties=prop, size = 30, color='#00d95a')\n",
    "        assi.set_ylabel('ECCDF'.format(fname), fontproperties=prop, size = 30, color='#00d95a')\n",
    "\n",
    "        #Salvo il grafico in un png in modo tale da mostrarlo nel powerpoint della presentazione\n",
    "        plt.savefig(f'global_ECDF{z}.png')\n",
    "        #plt.savefig(f'viral_ECDF{z}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contare la frequenza delle canzoni nei vari giorni di data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "\n",
    "def count_titles_in_csv_files():\n",
    "    # Dizionario per memorizzare il conteggio dei titoli\n",
    "    title_count = {}\n",
    "\n",
    "    # Cerca tutti i file CSV nella stessa cartella del sorgente Python che iniziano con \"z-global.csv\"\n",
    "    for i in range(1,16):\n",
    "        #with open(f'0{i}-global.csv', 'r', encoding = 'utf-16') as csvfile:\n",
    "        with open(f'0{i}-viral.csv', 'r', encoding = 'utf-16') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                # Ottiene il titolo dalla riga\n",
    "                title = row[2:]\n",
    "                # Aggiorna il conteggio del titolo nel dizionario\n",
    "                title_count[title[0]] = title_count.get(title[0], 0) + 1\n",
    "\n",
    "    return title_count\n",
    "\n",
    "# Ottieni il conteggio dei titoli nei file CSV\n",
    "titles_count = count_titles_in_csv_files()\n",
    "\n",
    "sorted_titles_count = dict(sorted(titles_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Stampare il conteggio dei titoli\n",
    "for title, count in sorted_titles_count.items():\n",
    "    print(f\"{title}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
